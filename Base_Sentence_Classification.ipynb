{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Logistic Regression, Linear SVM, and Naive Bayes\n",
    "This notebook reads training and testing data from CSV files, preprocesses the text data, and performs classification using Logistic Regression, Linear SVM, and Naive Bayes with TF-IDF vectors. It reports the macro and micro F1 scores, accuracy, precision, recall, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data\n",
    "train_df = pd.read_csv('Dataset/preprocessed/part1/train.csv')\n",
    "val_df = pd.read_csv('Dataset/preprocessed/part1/val.csv')\n",
    "test_df = pd.read_csv('Dataset/preprocessed/part1/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 20, 21)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberize_labels(df, labels):\n",
    "    for label in labels:\n",
    "        df[label] = df[label].apply(lambda x: 1 if x == 'met' else 0)\n",
    "    return df\n",
    "\n",
    "train_df = numberize_labels(train_df, labels=['abdominal', 'creatinine', 'major_diabetes'])\n",
    "test_df = numberize_labels(test_df, labels=['abdominal', 'creatinine', 'major_diabetes'])\n",
    "val_df = numberize_labels(val_df, labels=['abdominal', 'creatinine', 'major_diabetes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine train and val for this part\n",
    "train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and vectorize text data\n",
    "def preprocess_data(train_df, test_df, text_column):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train_df[text_column])\n",
    "    X_test = vectorizer.transform(test_df[text_column])\n",
    "    return X_train, X_test, vectorizer\n",
    "\n",
    "# Preprocess data\n",
    "X_train, X_test, vectorizer = preprocess_data(train_df, test_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate model\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to load data, train and evaluate models for each label\n",
    "labels = ['abdominal', 'creatinine', 'major_diabetes']\n",
    "preds_lr = np.zeros((len(test_df), len(labels)))\n",
    "preds_svm = np.zeros((len(test_df), len(labels)))\n",
    "preds_nb = np.zeros((len(test_df), len(labels)))\n",
    "\n",
    "\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    y_train = train_df[label]\n",
    "    y_test = test_df[label]\n",
    "        \n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "    preds_lr[:, i] = train_evaluate_model(X_train, X_test, y_train, y_test, lr_model)\n",
    "        \n",
    "    svm_model = SVC(kernel='linear')\n",
    "    preds_svm[:, i] = train_evaluate_model(X_train, X_test, y_train, y_test, svm_model)\n",
    "        \n",
    "    nb_model = MultinomialNB()\n",
    "    preds_nb[:, i] = train_evaluate_model(X_train, X_test, y_train, y_test, nb_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Evaluation\n",
      "(21, 3) (21, 3)\n",
      "{'accuracy': 0.6507936507936508, 'precision': array([0.        , 1.        , 0.73684211]), 'recall': array([0.        , 0.22222222, 1.        ]), 'micro_f1': 0.5925925925914952, 'macro_f1': 0.4040404040102847, 'confusion_matrix': array([[25,  6],\n",
      "       [16, 16]], dtype=int64)}\n",
      "SVM Evaluation\n",
      "(21, 3) (21, 3)\n",
      "{'accuracy': 0.7142857142857143, 'precision': array([0.75, 1.  , 0.75]), 'recall': array([0.66666667, 0.22222222, 0.85714286]), 'micro_f1': 0.689655172412604, 'macro_f1': 0.6231729054759779, 'confusion_matrix': array([[25,  6],\n",
      "       [12, 20]], dtype=int64)}\n",
      "Naive Bayes Evaluation\n",
      "(21, 3) (21, 3)\n",
      "{'accuracy': 0.6031746031746031, 'precision': array([0.        , 0.        , 0.66666667]), 'recall': array([0., 0., 1.]), 'micro_f1': 0.5283018867914561, 'macro_f1': 0.2666666666491429, 'confusion_matrix': array([[24,  7],\n",
      "       [18, 14]], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(preds, labels):\n",
    "    \n",
    "   \n",
    "\n",
    "    # preds = preds.int().numpy()\n",
    "    labels = labels.to_numpy()\n",
    "    print(labels.shape, preds.shape)\n",
    "\n",
    "    \n",
    "    # Initialize counts\n",
    "    tp = np.zeros((3,))\n",
    "    tn = np.zeros((3,))\n",
    "    fp = np.zeros((3,))\n",
    "    fn = np.zeros((3,))\n",
    "    \n",
    "    for i in range(3):\n",
    "        tp[i] = np.sum((preds[:, i] == 1) & (labels[:, i] == 1))\n",
    "        tn[i] = np.sum((preds[:, i] == 0) & (labels[:, i] == 0))\n",
    "        fp[i] = np.sum((preds[:, i] == 1) & (labels[:, i] == 0))\n",
    "        fn[i] = np.sum((preds[:, i] == 0) & (labels[:, i] == 1))\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = tp / (tp + fp + 1e-10)\n",
    "    recall = tp / (tp + fn + 1e-10)\n",
    "    micro_f1 = 2 * np.sum(tp) / (2 * np.sum(tp) + np.sum(fp) + np.sum(fn) + 1e-10)\n",
    "    macro_f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels.flatten(), preds.flatten()),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': np.mean(macro_f1),\n",
    "        'confusion_matrix': confusion_matrix(labels.flatten(), preds.flatten())\n",
    "    }\n",
    "\n",
    "print(\"Logistic Regression Evaluation\")\n",
    "print(compute_metrics(preds_lr, test_df[labels]))\n",
    "print(\"SVM Evaluation\")\n",
    "print(compute_metrics(preds_svm, test_df[labels]))\n",
    "print(\"Naive Bayes Evaluation\")\n",
    "print(compute_metrics(preds_nb, test_df[labels]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
