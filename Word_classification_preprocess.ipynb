{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, lowercase=True, stopword_removal=True, stopwords_domain=[], min_length=2,  punctuation_removal=True,\n",
    "                    does_stem=False, does_lemm=False):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if punctuation_removal:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    if stopword_removal:\n",
    "        stop_words = set(stopwords.words('english') + stopwords_domain)\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    if does_stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    if does_lemm:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if len(word) >= min_length]\n",
    "\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n"
     ]
    }
   ],
   "source": [
    "path = \"Dataset/n2c2/part2/\"\n",
    "files = [Path(f).stem for f in listdir(path=path) if isfile(join(path, f))]\n",
    "files = list(set(files))\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(txt_file):\n",
    "    with open(txt_file, 'r') as f:\n",
    "        text = f.read()\n",
    "        return text\n",
    "\n",
    "pattern = r'^(T\\d+)\\s+(\\w+)\\s+(\\d+)\\s+(\\d+)\\s+(.*)$'\n",
    "def extract_ann(ann_file):\n",
    "    with open(ann_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        all_anns = []\n",
    "        for line in lines:\n",
    "            match = re.match(pattern, line)\n",
    "            if match:\n",
    "                entry = {}\n",
    "                entry['entity_id'] = match.group(1)\n",
    "                entry['entity_type'] = match.group(2)\n",
    "                entry['start_span'] = int(match.group(3))\n",
    "                entry['end_span'] = int(match.group(4))\n",
    "                entry['name_or_dosage'] = match.group(5)\n",
    "                all_anns.append(entry)\n",
    "        return all_anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting and Cleaning Dataset: 100%|██████████| 303/303 [00:07<00:00, 38.27it/s]\n"
     ]
    }
   ],
   "source": [
    "### now extracting data \n",
    "dataset = []\n",
    "\n",
    "for file in tqdm(files, desc=\"Extracting and Cleaning Dataset\"):\n",
    "    data = {}\n",
    "    txt = extract_text(path + file + \".txt\")\n",
    "    #TODO doing the desired preprocessing on txt\n",
    "    txt = preprocess_text(text=txt, lowercase=True, stopword_removal=True, min_length=2,\n",
    "                           punctuation_removal=True, does_stem=False, does_lemm=False)\n",
    "    data['text'] = txt\n",
    "    anns = extract_ann(path + file + \".ann\")\n",
    "    #TODO doing the desired preprocessing on anns\n",
    "    for ann in anns:\n",
    "        ann['name_or_dosage'] = preprocess_text(text=ann['name_or_dosage'], lowercase=True, min_length=1)\n",
    "\n",
    "    data['anns'] = anns\n",
    "    \n",
    "    dataset.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>anns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>admission date 2200518 discharge date 2200530 ...</td>\n",
       "      <td>[{'entity_id': 'T1', 'entity_type': 'Drug', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>admission date 21471012 discharge date 2147102...</td>\n",
       "      <td>[{'entity_id': 'T3', 'entity_type': 'Drug', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>admission date 2170105 discharge date 21701016...</td>\n",
       "      <td>[{'entity_id': 'T1', 'entity_type': 'Reason', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>admission date 2136718 discharge date 213681 s...</td>\n",
       "      <td>[{'entity_id': 'T1', 'entity_type': 'Drug', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>admission date 219951 discharge date 219958 da...</td>\n",
       "      <td>[{'entity_id': 'T1', 'entity_type': 'Drug', 's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  admission date 2200518 discharge date 2200530 ...   \n",
       "1  admission date 21471012 discharge date 2147102...   \n",
       "2  admission date 2170105 discharge date 21701016...   \n",
       "3  admission date 2136718 discharge date 213681 s...   \n",
       "4  admission date 219951 discharge date 219958 da...   \n",
       "\n",
       "                                                anns  \n",
       "0  [{'entity_id': 'T1', 'entity_type': 'Drug', 's...  \n",
       "1  [{'entity_id': 'T3', 'entity_type': 'Drug', 's...  \n",
       "2  [{'entity_id': 'T1', 'entity_type': 'Reason', ...  \n",
       "3  [{'entity_id': 'T1', 'entity_type': 'Drug', 's...  \n",
       "4  [{'entity_id': 'T1', 'entity_type': 'Drug', 's...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"Dataset/preprocessed/part2.csv\", index=False)\n",
    "\n",
    "output_json = \"Dataset/preprocessed/part2.json\"\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(dataset, f, indent=4)\n",
    "\n",
    "print(\"Dataset saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset/preprocessed/part2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>anns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>admission date 2200518 discharge date 2200530 ...</td>\n",
       "      <td>[{'entity_id': 'T1', 'entity_type': 'Drug', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>admission date 21471012 discharge date 2147102...</td>\n",
       "      <td>[{'entity_id': 'T3', 'entity_type': 'Drug', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>admission date 2170105 discharge date 21701016...</td>\n",
       "      <td>[{'entity_id': 'T1', 'entity_type': 'Reason', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>admission date 2136718 discharge date 213681 s...</td>\n",
       "      <td>[{'entity_id': 'T1', 'entity_type': 'Drug', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>admission date 219951 discharge date 219958 da...</td>\n",
       "      <td>[{'entity_id': 'T1', 'entity_type': 'Drug', 's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  admission date 2200518 discharge date 2200530 ...   \n",
       "1  admission date 21471012 discharge date 2147102...   \n",
       "2  admission date 2170105 discharge date 21701016...   \n",
       "3  admission date 2136718 discharge date 213681 s...   \n",
       "4  admission date 219951 discharge date 219958 da...   \n",
       "\n",
       "                                                anns  \n",
       "0  [{'entity_id': 'T1', 'entity_type': 'Drug', 's...  \n",
       "1  [{'entity_id': 'T3', 'entity_type': 'Drug', 's...  \n",
       "2  [{'entity_id': 'T1', 'entity_type': 'Reason', ...  \n",
       "3  [{'entity_id': 'T1', 'entity_type': 'Drug', 's...  \n",
       "4  [{'entity_id': 'T1', 'entity_type': 'Drug', 's...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
